{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "#get the openai key using os taking from the env\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bot_assistant.overview_summarizer import summarize_dataframe\n",
    "filtered_well_data = pd.read_csv('npd_overall/Explo_and_Dev_concat_wells.csv')\n",
    "summary_response = summarize_dataframe(filtered_well_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"You are an assistant that is given with a dataframe converted into a string. Your task is to do the following step by step:\\n 1. Convert the string into a dataframe\\n 2. Using the dataframe, provide the percentage distribution of the column wlbcontent using df[wlbcontent].value_counts(normalize=True) * 100 \\n 3. Convert the new dataframe back into string. \\n 4. Create a summary of the distribution.\"\n",
    "\n",
    "instruction2 =\"You are an assistant that is given with a dataframe converted into a string. Your only task is to proivde a summary of the 'wlbcontent' column and express it in percentage. keep the summary concise and direct. just give an overview of the data not more than 5 sentences.\"\n",
    "\n",
    "instruction3 = \"You are an assistant that is given with a dataframe converted into a string. Your only task is to proivde a summary of the 'wlbcontent' column and express it in percentage. keep the summary concise and direct. just give an overview of the data not more than 5 sentences.\"\n",
    "instruction4= \"Given the data containing value counts as percentages for each column in a DataFrame, please provide a brief summary highlighting the most significant findings. Focus on key trends, dominant values, or notable disparities in the data across different columns.\"\n",
    "\n",
    "instruction5= \"Given the data containing value counts as percentages for each column in a DataFrame, please provide a brief summary highlighting the most significant findings. Focus on key trends, dominant values, or notable disparities in the data across different columns. Adjust your summary based on the user query.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "def summarize_dataframe(df):\n",
    "    GPT_MODEL = \"gpt-3.5-turbo-1106\"  # Replace with your desired model\n",
    "    \n",
    "    columns = ['wlbDrillingOperator', 'wlbPurpose', 'wlbStatus', 'wlbContent', 'wlbWellType']\n",
    "    normalized_counts = {column: (df[column].value_counts(normalize=True) * 100).round(2) for column in columns}\n",
    "    data_string = str(normalized_counts)\n",
    "\n",
    "    instruction = \"Given the data containing value counts as percentages for each column in a DataFrame, please provide a brief summary highlighting the most significant findings. Focus on key trends, dominant values, or notable disparities in the data across different columns.\"\n",
    "    message = \"This is data:\\n\\n\" + data_string\n",
    "    prompt_message = [{\"role\": \"system\", \"content\": instruction}, {\"role\": \"assistant\", \"content\": message}]\n",
    "\n",
    "    # Ensure you have set your API key in your environment or replace 'YOUR_API_KEY' with your actual key\n",
    "    openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        temperature=0.0,\n",
    "        messages=prompt_message,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the key findings from the data:\n",
      "\n",
      "1. **Drilling Operators**: Statoil Petroleum AS, Norsk Hydro Produksjon AS, and Den norske stats oljeselskap a.s are the top three drilling operators, accounting for approximately 40% of the total operations.\n",
      "\n",
      "2. **Well Purpose**: The majority of wells are drilled for production purposes (53.54%), followed by wildcat exploration (17.22%) and injection (11.35%).\n",
      "\n",
      "3. **Well Status**: A significant portion of wells are either plugged (32.45%) or permanently abandoned (27.64%), indicating a substantial number of completed or inactive wells.\n",
      "\n",
      "4. **Well Content**: The most common well content is oil (47.93%), followed by gas (9.89%) and a combination of oil and gas (6.93%). A notable percentage (7.64%) is labeled as \"not applicable.\"\n",
      "\n",
      "5. **Well Type**: The majority of wells are classified as development wells (73.13%), indicating a focus on expanding existing fields rather than pure exploration.\n",
      "\n",
      "Overall, the data suggests a strong emphasis on production drilling, with a significant presence of oil-focused wells and a substantial number of completed or inactive wells.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-1106\"#gpt-4-1106-preview\"\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('npd_overall/Explo_and_Dev_concat_wells.csv')\n",
    "#df=df.head(30)\n",
    "# Process your data - here we're just converting it to a string, but you might want to summarize or extract info\n",
    "#data_string = df.to_string()\n",
    "instruction4= \"Given the data containing value counts as percentages for each column in a DataFrame, please provide a brief summary highlighting the most significant findings. Focus on key trends, dominant values, or notable disparities in the data across different columns.\"\n",
    "columns = [ 'wlbDrillingOperator',\n",
    "       'wlbPurpose', 'wlbStatus', 'wlbContent',\n",
    "       'wlbWellType']\n",
    "\n",
    "normalized_counts = {}\n",
    "for column in columns:\n",
    "    # Calculate normalized value counts for each column\n",
    "    normalized_counts[column] = (df[column].value_counts(normalize=True) * 100).round(2)\n",
    "data_string = str(normalized_counts )\n",
    "\n",
    "\n",
    "message= \"This is data:\\n\\n\" + data_string\n",
    "prompt_message = [{\"role\": \"system\", \"content\": instruction4},{\"role\": \"assistant\", \"content\":message}]\n",
    "\n",
    "\n",
    "# Define your API key\n",
    "# Send a request to the OpenAI API\n",
    "response = openai.ChatCompletion.create(\n",
    "            model=GPT_MODEL,\n",
    "      \n",
    "            temperature=0.0,\n",
    "            messages=prompt_message,\n",
    "            max_tokens=500,\n",
    "        )\n",
    "\n",
    "summary_response= response['choices'][0]['message']['content']\n",
    "\n",
    "# Print the response\n",
    "print(response['choices'][0]['message']['content'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the df, create a pecentage disctribution based on the content column\n",
    "df = pd.read_csv('npd_overall/Explo_and_Dev_concat_wells.csv')\n",
    "df=df.head(30)\n",
    "columns = [ 'wlbDrillingOperator',\n",
    "       'wlbPurpose', 'wlbStatus', 'wlbContent',\n",
    "       'wlbWellType']\n",
    "#df[['wlbContent', 'wlbPurpose']].value_counts(normalize=True) * 100\n",
    "normalized_counts = {}\n",
    "for column in columns:\n",
    "    # Calculate normalized value counts for each column\n",
    "    normalized_counts[column] = (df[column].value_counts(normalize=True) * 100).round(2)\n",
    "#convert dict to dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRY: 20.0%\\n- GAS/CONDENSATE:\n",
    " 13.33%\\n- OIL: 26.67%\\n- OIL SHOWS: 13.33%\\n- GAS: 6.67%\\n- SHOWS: 13.33%\\n- NOT APPLICABLE: 6.67%\\n\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.insert(1, '/Users/2924441/Desktop/phd part 2/add_fm_data')\n",
    "from utils.add_fm_data import add_formation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fm_data = pd.read_csv('formation_data/F-15A.csv', sep=';')\n",
    "fm_data.drop(fm_data.columns[fm_data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "fm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ddata = pd.read_csv('drilling_data/F-11A_time_8.5in.csv', sep=',')\n",
    "ddata.drop(ddata.columns[ddata.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "ddata['Well_name'] = 'F-11A'\n",
    "ddata.to_csv('drilling_data/F-11A_time_8.5in.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fm_measured_depth = fm_data['m MD']\n",
    "fm = fm_data['Formation Tops']\n",
    "ddata[\"formation\"] = None\n",
    "fm_and_depth = list(zip(fm_measured_depth[:-1],fm_measured_depth[1:],fm[:-1]))\n",
    "\n",
    "for i in fm_and_depth:\n",
    "    ddata.loc[(ddata['HoleDepth(m)'] >=i[0]) &  (ddata['HoleDepth(m)']<=i[1]), 'formation'] = i[2]\n",
    "\n",
    "ddata.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.viz_wellpath import viz_wellpath_plot\n",
    "viz_wellpath_plot(['F-15', 'F-15A' ,'F-15B','F-15C' ,'F-15D' ,'F-1C' ,'F-11A' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.viz_formation import viz_formation_with_depth, viz_rop_formation\n",
    "viz_rop_formation(['F-15A','F-1C','F-11A' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.daily_operation_report import daily_report_sun\n",
    "daily_report_sun('15_9-F_15')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.viz_wellpath import viz_wellpath_plot\n",
    "viz_wellpath_plot(['15_9-F-15', '15_9-F-15 A' ,'15_9-F-15 B','15_9-F-15 C' ,'15_9-F-15 D' ,'15_9-F-1 C' ,'15_9-F-11 A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_wellpath_plot([ '25_4-7','25_4_K-7 H' ,'25_4_K-7 AY1H' ,'25_4_K-7 AY2H','25_4-K-7 AY3H' ,'25_4_K-7 AY1HT5','25_4_K-7 AY1HT6' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_wellpath_plot(['25_4-7','25_4_K-7 H' ,'25_4_K-7 AY1H' ,'25_4_K-7 AY2H','25_4-K-7 AY3H' ,'25_4_K-7 AY1HT5','25_4_K-7 AY1HT6' ,'15_9-F-15' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ave_ROP_Depth import viz_averop_formation\n",
    "viz_averop_formation(['15_9-F-11 A','15_9-F-15 A','15_9-F-1 C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.viz_formation import viz_rop_formation\n",
    "viz_rop_formation(['15_9-F-11 A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_rop_formation(['15_9-F-15 A','15_9-F-1 C','15_9-F-11 A'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre process aker bp drilling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def change_file_names_drilling_data(dir_drilling_data_source, drilling_data_destination):\n",
    "    \"\"\"change file names from Adrians format.\n",
    "    Example:\n",
    "    25_4-K-7 AY1HT5_Surface_Time_17.5in_Section.csv\n",
    "    converted to \n",
    "    25_4-K-7 AY1HT5_17.5in.csv\n",
    "     \n",
    "\n",
    "    :param _type_ dir_drilling_data: _description_\n",
    "    \"\"\"\n",
    "    list_of_files = os.listdir(dir_drilling_data_source)\n",
    "\n",
    "    for file_name in list_of_files:\n",
    "        if 'Surface_Time' and '_Section'in file_name:\n",
    "            old_name = dir_drilling_data_source+'/'+file_name\n",
    "\n",
    "            file_name= file_name.replace('Surface_Time', \"time\" )\n",
    "            file_name= file_name.replace('_Section', \"\" )\n",
    "            \n",
    "            new_name = drilling_data_destination +'/' + file_name\n",
    "            os.rename(old_name, new_name)\n",
    "\n",
    "change_file_names_drilling_data(dir_drilling_data_source = '/Users/2924441/Desktop/phd part 2/add_fm_data/aker_bp_data/drilling_data_Adrian',\n",
    "                drilling_data_destination = '/Users/2924441/Desktop/phd part 2/add_fm_data/aker_bp_data/drilling_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_rename_drilling_data(dir_drilling_data):\n",
    "\n",
    "    list_of_files = os.listdir(dir_drilling_data)\n",
    "    for file_name in list_of_files:\n",
    "        if '.csv' in file_name:\n",
    "            file_path =dir_drilling_data + '/' + file_name\n",
    "            print(file_name )\n",
    "            my_aker_dd_data = pd.read_csv(file_path, sep=',')\n",
    "        \n",
    "            try:\n",
    "                my_aker_dd_data.rename(columns = {'HDEP - m':'HoleDepth(m)'}, inplace = True)\n",
    "                my_aker_dd_data.rename(columns = {'DEP - m':'Bit Measured Depth m'}, inplace = True)\n",
    "                my_aker_dd_data.rename(columns = {'ROPI - m/h':'Inst ROP(m/h)'}, inplace = True)\n",
    "                my_aker_dd_data.rename(columns = {'ROPA - m/h':'Time Averaged ROP m/h'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['RPMBAVG - rpm'] = my_aker_dd_data['RPMBAVG - rpm']/60\n",
    "                my_aker_dd_data.rename(columns = {'RPMBAVG - rpm':'RPM total ave c s'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['RPMSAVG - rpm'] = my_aker_dd_data['RPMSAVG - rpm']/60\n",
    "                my_aker_dd_data.rename(columns = {'RPMSAVG - rpm':'Surface RPM c s'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['TQABAV - kN.m'] = my_aker_dd_data['TQABAV - kN.m']/1000\n",
    "                my_aker_dd_data.rename(columns = {'TQABAV - kN.m':'SurfaceTorque(N.m)'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['DMIAVG - g/cm3'] = my_aker_dd_data['DMIAVG - g/cm3']*1000\n",
    "                my_aker_dd_data.rename(columns = {'DMIAVG - g/cm3':'Mud Weight In kg m3'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['FLIAVG - L/min'] = my_aker_dd_data['FLIAVG - L/min']/60000\n",
    "                my_aker_dd_data.rename(columns = {'FLIAVG - L/min':'FlowRateIn(m3/s)'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data.rename(columns = {'RISFLO - L/min':'Riser flow ave L/min'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['SPPAVG - bar'] = my_aker_dd_data['SPPAVG - bar']*100000\n",
    "                my_aker_dd_data.rename(columns = {'SPPAVG - bar':'Pump Pressure - Stand Pipe Pa'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data['HKLDAV - N'] = my_aker_dd_data['HKLDAV - N']/9.81\n",
    "                my_aker_dd_data.rename(columns = {'HKLDAV - N':'Hookload(kg)'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data.rename(columns = {'WOBAVG - N':'Weight On Bit N'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data.rename(columns = {'BSZ - in':'bit_size (in)'}, inplace = True)\n",
    "\n",
    "                my_aker_dd_data.to_csv(file_path, sep=',')\n",
    "                print('saved: ', file_name )\n",
    "            \n",
    "            except:\n",
    "                print(file_name + ' was not reformatted')\n",
    "\n",
    "convert_and_rename_drilling_data('/Users/2924441/Desktop/phd part 2/add_fm_data/aker_bp_data/drilling_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/2924441/Desktop/phd part 2/add_fm_data/all_drill_with_fm_csv'+'/'+'15_9-F-1 C'+'.csv', sep=',')\n",
    "dff=df\n",
    "dff = dff.groupby(['Well_name', 'formation'], as_index = False).agg(\n",
    "                {'HoleDepth(m)': ['mean', 'min', 'max'],'Time Averaged ROP m/h':['mean']}).round(2)\n",
    "dff['MD_per_drilled_fm'] =(dff['HoleDepth(m)']['max'] - dff['HoleDepth(m)']['min']).round(2)\n",
    "\n",
    "dff_columns = list(map(lambda x: x[0] if x[1] =='' else ' '.join(x), dff.columns.values))\n",
    "\n",
    "dff_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "# Creating the low level functional client\n",
    "client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id = 'AKIA2TIB54KEQPXUD7AZ',\n",
    "    aws_secret_access_key = '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',\n",
    "    region_name = 'us-east-1'\n",
    ")\n",
    "    \n",
    "# Creating the high level object oriented interface\n",
    "resource = boto3.resource(\n",
    "    's3',\n",
    "    aws_access_key_id = 'AKIA2TIB54KEQPXUD7AZ',\n",
    "    aws_secret_access_key = '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',\n",
    "    region_name = 'us-east-1'\n",
    ")\n",
    "\n",
    "\n",
    "# Create the S3 object\n",
    "obj = client.get_object(\n",
    "    Bucket = 'paciswelldata',\n",
    "    Key = 'all_drill_with_fm_csv/15_9-F-1 C.csv'\n",
    ")\n",
    "print(obj)\n",
    "# Read data from the S3 object\n",
    "data = pd.read_csv(obj['Body'])\n",
    "    \n",
    "# Print the data frame\n",
    "print('Printing the data frame...')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.arange(0.0, 2.0, 0.01)\n",
    "s = 1 + np.sin(2 * np.pi * t)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "ax.plot(t, s)\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='voltage (mV)',\n",
    "       title='About as simple as it gets, folkssss')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"test.png\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "client.upload_file(\"test.png\",\n",
    "    Bucket = 'paciswelldata',\n",
    "    Key = 'images/2.png'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# client.put_object(\n",
    "#         Bucket=bucket,\n",
    "#         Body=f,\n",
    "#         Key=key\n",
    "#     )\n",
    "\n",
    "# tmpkey = key.replace('/', '')\n",
    "# download_path = '/tmp/{}{}'.format(uuid.uuid4(), tmpkey)\n",
    "# upload_path = '/tmp/resized-{}'.format(tmpkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = client.get_object(\n",
    "    Bucket = 'paciswelldata',\n",
    "    Key = 'all_drill_with_fm_csv/15_9-F-1 C.csv'\n",
    ")\n",
    "data = pd.read_csv(obj['Body'])\n",
    "    \n",
    "# Print the data frame\n",
    "print('Printing the data frame...')\n",
    "data['Time Averaged ROP m/h'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"My name is {}, I'm {}\".format(\"John\",36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = client.get_object(\n",
    "    Bucket = 'paciswelldata',\n",
    "    Key = 'all_drill_with_fm_csv/15_9-F-1 C.csv'\n",
    ")\n",
    "#data = pd.read_csv(obj['Body'])\n",
    "file_reader = obj['Body'].read().decode(\"utf-8\")\n",
    "# Print the data frame\n",
    "print('Printing the data frame...')\n",
    "#data\n",
    "file_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/2924441/Desktop/phd part 2/add_fm_data/all_drill_with_fm_csv/15_9-F-1 C.csv')\n",
    "    \n",
    "# Print the data frame\n",
    "print('Printing the data frame...')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/2924441/Desktop/phd part 2/add_fm_data/npd_overall/Explo_and_Dev_concat_wells.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/2924441/Desktop/phd part 2/add_fm_data/npd_overall/Explo_and_Dev_concat_wells.csv')\n",
    "\n",
    "def convert_val_counts_dict(column_name):\n",
    "    values = df[column_name].value_counts(dropna=False).keys().tolist()\n",
    "    counts = df[column_name].value_counts(dropna=False).tolist()\n",
    "    return [values, counts]\n",
    "    \n",
    "# well_type_dicts =  convert_val_counts_dict('wlbWellType')\n",
    "# operator_dicts = convert_val_counts_dict('wlbDrillingOperator')\n",
    "# content_dicts = convert_val_counts_dict('wlbContent')\n",
    "# purpose_dicts = convert_val_counts_dict('wlbPurpose')\n",
    "# status_dicts = convert_val_counts_dict('wlbStatus')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['wlbWellType','wlbDrillingOperator'])['wlbDrillingOperator'].count()#.to_dict()\n",
    "convert_val_counts_dict('wlbContent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subplots\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/Users/2924441/Desktop/phd part 2/add_fm_data/npd_overall/Explo_and_Dev_concat_wells.csv')\n",
    "\n",
    "well_type_dicts = df['wlbWellType'].to_dict()\n",
    "\n",
    "bar_plots_names = [ 'wlbWellType','wlbDrillingOperator','wlbPurpose', 'wlbStatus', 'wlbContent']\n",
    "#['Well type', 'Operator' , 'Purpose' ,'Status' ,'Content']\n",
    "fig = make_subplots(rows=5, cols=1,subplot_titles=('Well type', 'Operator' , 'Purpose' ,'Status' ,'Content'))\n",
    "\n",
    "for i, name, in enumerate(bar_plots_names,1):\n",
    "    y,x = convert_val_counts_dict(name)\n",
    "    fig.append_trace(go.Bar(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            orientation='h'), row=i, col=1)\n",
    "\n",
    "\n",
    "\n",
    "fig.update_layout(height=600, width=600, title_text=\"Stacked Subplots\", showlegend=False)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "labels = ['wlbDrillingOperator','wlbPurpose', 'wlbStatus', 'wlbContent', 'wlbWellType']\n",
    "widths = np.array([10,10,10,10,10])\n",
    "\n",
    "data = {\n",
    "    \"Dev\": [50,80,60,70,30],\n",
    "    \"Explr\": [50,20,40,30,30]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "labels = [\"apples\",\"oranges\",\"pears\",\"bananas\"]\n",
    "widths = np.array([10,20,20,50])\n",
    "\n",
    "data = {\n",
    "    \"South\": [50,80,60,70],\n",
    "    \"North\": [50,20,40,30]\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "for key in data:\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=key,\n",
    "        y=data[key],\n",
    "        x=np.cumsum(widths)-widths,\n",
    "        width=widths,\n",
    "        offset=0,\n",
    "        customdata=np.transpose([labels, widths*data[key]]),\n",
    "        texttemplate=\"%{y} x %{width} =<br>%{customdata[1]}\",\n",
    "        textposition=\"inside\",\n",
    "        textangle=0,\n",
    "        textfont_color=\"white\",\n",
    "        hovertemplate=\"<br>\".join([\n",
    "            \"label: %{customdata[0]}\",\n",
    "            \"width: %{width}\",\n",
    "            \"height: %{y}\",\n",
    "            \"area: %{customdata[1]}\",\n",
    "        ])\n",
    "    ))\n",
    "\n",
    "fig.update_xaxes(\n",
    "    tickvals=np.cumsum(widths)-widths/2,\n",
    "    ticktext= [\"%s<br>%d\" % (l, w) for l, w in zip(labels, widths)]\n",
    ")\n",
    "\n",
    "fig.update_xaxes(range=[0,100])\n",
    "fig.update_yaxes(range=[0,100])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Marimekko Chart\",\n",
    "    barmode=\"stack\",\n",
    "    uniformtext=dict(mode=\"hide\", minsize=10),\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_columns = ['wlbWellboreName', 'wlbEntryDate','wlbDrillingOperator','wlbTotalDepth',\n",
    "                'wlbProductionLicence', 'wlbPurpose', 'wlbStatus', 'wlbContent',\n",
    "                'wlbWellType',  'wlbNsDecDeg', 'wlbEwDesDeg']\n",
    "edited_columns = ['Wellbore Name', 'Entry Date','Drilling Operator','Total Depth (MD)[m RKB]',\n",
    "                'Production Licence', 'Purpose', 'Status', 'Content',\n",
    "                'Well Type',  'Ns Deg', 'Ew Deg']\n",
    "new_names_of_columns = dict(zip(table_columns,edited_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "def convert_survey_txt_to_csv(filepath,well_name,csv_directory):\n",
    "    \"\"\"convert survey file from txt file to csv from volve\n",
    "\n",
    "    :param _type_ filepath: _description_\n",
    "    :param _type_ well_name: _description_\n",
    "    :param _type_ csv_directory: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    f = open(filepath, 'r', encoding = \"ISO-8859-1\")\n",
    "\n",
    "    content = f.readlines()\n",
    "    for i in range(len(content)):\n",
    "        if ''.join(content[i].split()) == 'SURVEYLIST':\n",
    "            survey = i\n",
    "            print(survey)\n",
    "            break\n",
    "    lists_list = []\n",
    "    for line in content[i+1:]:\n",
    "            line = line.strip()\n",
    "            # string with multiple consecutive spaces\n",
    "            s = line.replace('\\n',\"\")\n",
    "            # make spaces consistent\n",
    "            s = re.sub(\"  +\", \"*\", s)\n",
    "            s = s.split('*')\n",
    "            assert len(s) ==9\n",
    "            lists_list.append(s)\n",
    "    df = pd.DataFrame(lists_list[2:], columns =[m+'('+n+')' for m,n in zip(lists_list[0],lists_list[1])])\n",
    "    return df\n",
    "    # df.to_csv(csv_directory+'/'+well_name+'.csv', sep=',')\n",
    "    # print('saved:', well_name)\n",
    "\n",
    "\n",
    "survet_text = '/Users/2924441/Desktop/equinor volve azure/survey/15_9-F-11 A'\n",
    "df =convert_survey_txt_to_csv(survet_text,'well_name','csv_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.daily_operation_report import daily_report_sun\n",
    "daily_report_sun('15_9-F_15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.load(open(\"/Users/2924441/Desktop/phd part 2/add_fm_data/npd_overall/dev_wells_info.pkl\", \"rb\"))['1_3-A-1 H'].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WellExApp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6021bc6a7a432ed6052756775922025bb526882d7e1fb37d209042d29f9809fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
